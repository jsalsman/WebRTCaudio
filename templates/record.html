<!DOCTYPE html>
<html lang="en">
  <!--
      webrec: Record and Upload Audio with Stop on Silence
  
      This Python Flask HTML template, part of the webrec application, 
      serves as the main user interface for recording and uploading audio
      with an additional feature to stop recording on silence detection. 
      The page includes styles, scripts for voice activity detection, and
      handling of audio recording functionalities.

      The interface consists of controls for recording, visual indicators 
      for audio levels and the length of the recording, and options for 
      voice activity detection settings. JavaScript is extensively used 
      for handling audio recording, processing, and uploading functions,
      using the current WebRTC GetUserMedia and AudioWorklet APIs.

      The application uses external libraries for enhanced features like 
      voice activity detection in WebAssembly and audio processing in Pysox.

      This application is released under the MIT License, November 24, 2023.

      run: https://webrec.replit.app

      dev: https://replit.com/@jsalsman/webrec

      github: https://github.com/jsalsman/webrec
  -->
<head>
  <meta charset="UTF-8">
  <title>webrec: Record and Upload Audio</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="/static/microphone.png">

  <!-- Voice Activity Detection: @ricky0123/vad-web, see below -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.min.js"></script>

  <style>
    body {
      font-family: 'Verdana', sans-serif;
      background-color: #fafafa; /* very light gray */
      -webkit-text-size-adjust: 100%;
    }

    a {
      text-decoration: none; /* Removes underlining from links */
    }

    h1, h2, h3 {
      font-family: 'Georgia', serif;
    }

    button {
      font-family: 'Verdana', sans-serif;
      font-size: 16px;
      padding: 10px 20px; 
      border: 0; /* No border */
      border-radius: 5px; /* Rounds the corners */
      background-color: lightgreen; /* background */
      color: black; /* text */
      margin: 0; /* No margins */
      box-sizing: border-box; /* Ensures padding is included in the total width and height */
      -webkit-appearance: none; /* Removes default styling on iOS */
      appearance: none; /* Removes default styling on other browsers */
    }

    button:disabled {
      background-color: gray;
      color: white;
    }

    /* Flex container for meters */
    .meter-container {
      display: flex;
      flex-direction: row;
      align-items: center; /* Centers items vertically */
      gap: 15px;
      max-width: 500px; /* Maximum width of the container */
      margin-bottom: 20px; /* Adds some space below each meter */
    }

    meter {
      flex: 1; /* Allows the meter to grow */
      /* Remove inline styles for width from the HTML and control it here if necessary */
    }

    /* Flex container for buttons */
    .button-container {
      display: flex;
      flex-direction: row;
      gap: 15px;
      width: 100%; /* The container takes up 100% of the parent's width */
      max-width: 500px; /* Maximum width of the container */
      justify-content: space-between;
      margin-top: 20px; /* Adds some space above */
    }

    /* Styles for the buttons */
    #start, #startOver {
      flex: 1; /* Allows the buttons to grow and fill the container */
    }

    /* Style for the Safari help link container */
    #safari-help {
      margin-top: 20px; /* Adds some space above the help link */
    }
  </style>

</head>
<body>
  <h2>Record and Upload Audio with Stop on Silence</h2>

  <!-- VAD toggle and delay selection -->
  <div style="font-size: 120%;">
    <label><input type="checkbox" id="vad" checked> Stop</label> after:
    <label><input type="radio" name="sil_dur" value="1">&nbsp;1,</label>
    <label><input type="radio" name="sil_dur" value="2" checked>&nbsp;2,</label> or
    <label><input type="radio" name="sil_dur" value="3">&nbsp;3 seconds</label>
    of silence.
  </div>
  <br>
  
  <!-- Meter for audio level -->
  <div class="meter-container">
    <label for="level">Audio level:</label>
    <meter id="level" min="0" max="1" low="0.05" high="0.7" value="0"></meter>
    <span id="vadSpan">Silence</span>
  </div>

  <!-- Meter for recording time -->
  <div class="meter-container">
    <label for="full"><span id="secs">00.0</span> of up to 60 seconds:</label>
    <meter id="full" min="0" max="60" high="55" value="0"></meter>
  </div>

  <!-- Buttons -->
  <div class="button-container">
    <button id="start" disabled>Initializing...</button>
    <button id="startOver" style="display: none;">Start Over</button>
  </div>

  <!-- Safari users' help link -->
  <div id="safari-help" style="display: none;">
    <h3>Apple Safari users: <a 
      href="https://support.apple.com/guide/mac-help/control-access-to-the-microphone-on-mac-mchla1b1e1fe/mac"
      >please see here if you are having trouble.</a></h3>
  </div>
  <br>

  <!-- WebRTC noise supression toggle -->
  <div style="font-size: 120%">
    <span id="nsSpan"><label><input type="checkbox" id="suppressNoise">
      Noise supression (may degrade quality)</label></span>
  </div>

<script>
  // Global variables
  var context, gotContext = false;  // Audio context for managing audio
  // Stream for audio data, AudioWorkletNode from recording-processing.js
  var stream, recorder = null, isRecording = false;  // and recording flag
  // vad.MicVAD for Voice Activity Detection and Timeout for speech inactivity
  var myvad, stopped_speaking_timeout = null;

  // Function executed when the window loads
  window.onload = async function() {  
    // Check if the Web Audio API is supported by the browser
    if (!window.audioContext) {
      try {
        // Polyfill for AudioContext for wider browser support
        window.AudioContext = 
          window.AudioContext || window.webkitAudioContext;
        // Create a new AudioContext with a specific sample rate
        context = new AudioContext({ sampleRate: 16000 });
        // Check if the AudioContext is active
        if (context.state === 'running') {
          gotContext = true;
        } else {            
          // If AudioContext is suspended, it requires user interaction to activate
          document.getElementById('start').textContent = 'Detect Audio Levels';
          document.getElementById('start').disabled = false;
        }
      } catch (e) {
        // Handle the case where Web Audio API is not supported by the browser
        document.getElementById('start').disabled = true;
        alert('Web audio is not supported on this browser.');
      }
    } else {
      // Initialize the AudioContext for browsers that support it natively
      context = new AudioContext({ sampleRate: 16000 });
      // Check and update context status
      if (context.state === 'running') {
        gotContext = true;
      } else {
        // If AudioContext is suspended, enable it with user interaction
        document.getElementById('start').textContent = 'Detect Audio Levels';
        document.getElementById('start').disabled = false;
      }
    }
    
    // Use WebRTC API to check for microphone access
    navigator.mediaDevices.enumerateDevices().then(async devices => {
      // Check if any of the media devices are audio input devices (microphones)
      if (devices.some(device => device.kind === 'audioinput' && device.label)) {
        // The user has granted microphone access
        if (context.state === 'running') {
          await initRecorder();
        }
      } else {
        // When microphone access is not granted or no microphone is available
        if (/^((?!chrome|android).)*safari/i.test(navigator.userAgent)) {
          // Special help for Apple Safari browser users on microphone permissions
          document.getElementById('safari-help').style.display = 'block';
        }
        document.getElementById('start').textContent = 'Allow Microphone Use';
        document.getElementById('start').disabled = false;
      }
    });
    
    // Add event listener for the 'start' button
    document.getElementById('start').addEventListener('click', async () => {
      if (!recorder) {
        // If the recorder is not initialized, initialize it
        document.getElementById('start').disabled = true;
        document.getElementById('start').textContent = 'Initializing...';
        context.resume();
        await initRecorder();
      } else if (!isRecording) {
        // If not currently recording, start recording
        startRecording();
      } else {
        // If currently recording, stop recording
        stopRecording();
      }
    });
    
    // Add event listener for the 'startOver' button
    document.getElementById('startOver').onclick = function() {
      if (recorder && isRecording) { 
        // Send a message to reset the recording state if recording is in progress
        recorder.port.postMessage({ message: 'UPDATE_RECORDING_STATE', 
                       setRecording: true });  // Reset samples to zero
        clearTimeout(stopped_speaking_timeout);
      }
    }

    // Event listener for the noise suppression checkbox
    document.getElementById('suppressNoise').addEventListener('change', setNoiseSuppression);

  }

  // Function to initialize the audio recorder
  async function initRecorder() {
    try {
      // Use the WebRTC getUserMedia API to access the user's microphone
      stream = await navigator.mediaDevices.getUserMedia(
        { audio: {  // Set noise supression from the checkbox
            noiseSuppression: document.getElementById('suppressNoise').checked,
            autoGainControl: false,    // Optional automatic gain control
            latency: 0,                // Ask for minimum latency
            echoCancellation: false }, // Echo cancellation unnecessary here
         video: false });  // No video

      // Create a MediaStreamAudioSourceNode from the audio stream
      const micSourceNode = new MediaStreamAudioSourceNode(context,
          {mediaStream: stream});

      // Hide help for Apple Safari browser microphone permissions
      document.getElementById('safari-help').style.display = 'none';
      
      // Add a custom audio processor script to the AudioContext
      await context.audioWorklet.addModule('/static/recording-processor.js');
      // Create an AudioWorkletNode as the recorder
      recorder = new AudioWorkletNode(context, 'recording-processor',
        { processorOptions: {
          numberOfChannels: 1,
          sampleRate: 16000,
          maxFrameCount: 16000 * 60 // Maximum frames for 1 minute of audio
      }});

      // Connect the microphone source node to the recorder
      await micSourceNode.connect(recorder);

      // Handle messages from the audio worklet: /static/recording-processor.js
      recorder.port.onmessage = event => {

        if (event.data.message === 'SHARE_RECORDING_BUFFER') { 
          // Recording ended, upload file
          let length = event.data.recordingLength;
          console.log('Seconds Recorded: ' + length / 16000);

          // Convert the floating point array buffer to audio/l16 PCM
          let buffer = event.data.buffer[0];
          let pcmData = new Uint8Array(length * 2);
          for (let index = 0; index < length; ++index) {
            let sample = buffer[index];
            sample = sample * 32768.0;
            sample = Math.max(-32768, Math.min(32767, sample));
            pcmData[index * 2] = sample & 255; // low byte, little endian
            pcmData[index * 2 + 1] = (sample >> 8) & 255; // high byte
          }

          // Put the raw PCM data in a blob and upload it
          let formData = new FormData();
          let blob = new Blob([pcmData], { type: 'audio/l16' });
          formData.append('audio', blob, 'audio.raw');            
          fetch('/upload-audio', {
            method: 'POST',
            body: formData
          }).then(response => {
            if (response.ok) {
              window.location.href = response.url;
            }
          }).catch(error => {
            console.error('Upload error: ', e);
            alert('Upload error: ' + e);
            // Disallow further interaction
            document.getElementById('start').disabled = true;
            document.getElementById('startOver').disabled = true;
          });

        // Stop the recording if the buffer is full
        } else if (event.data.message === 'MAX_RECORDING_LENGTH_REACHED') {
          stopRecording();

        // Set the duration meter
        } else if (event.data.message === 'UPDATE_RECORDING_LENGTH') {
          let seconds = event.data.recordingLength / 16000
          document.getElementById('full').value = seconds.toFixed(2);
          document.getElementById('secs').textContent = 
            seconds.toFixed(1).padStart(4, '0');
          
        // Set the audio level meter
        } else if (event.data.message === 'UPDATE_VISUALIZERS') {
          let expKx = Math.exp(20 * event.data.gain);    // Don't ask me;
          document.getElementById('level').value =       // ChatGPT-4 came
            ((expKx - 1) / (expKx + Math.E)).toFixed(2); // up with this.
        }
      }
  
      // Voice Activity Detection: @ricky0123/vad-web
      // https://www.vad.ricky0123.com/docs/browser/

      // Suppress sub-warning "errors" in the console log from ort.js
      const origConsoleError = console.error; // Save original console.error()
      console.error = (...args) => { 
        // Check if the error message contains this specific text
        if (!args[0].includes("not used by any node and should be removed")) {
          origConsoleError(...args); // If not, call the regular console.error
        }
      };
      myvad = await vad.MicVAD.new({
        additionalAudioConstraints: { audio: stream },
        onSpeechStart: () => {
          //console.log('VAD: Speaking started');
          document.getElementById('vadSpan').textContent = 'Speech';
          document.getElementById('vadSpan').style.color = 'darkgreen';
          clearTimeout(stopped_speaking_timeout);
        },
        onSpeechEnd: (audio) => {
          //console.log('VAD: Speaking stopped');
          document.getElementById('vadSpan').textContent = 'Silence';
          document.getElementById('vadSpan').style.color = 'darkgray';
          if (isRecording && document.getElementById('vad').checked) {
            silence_secs = document.querySelector('input[name="sil_dur"]:checked').value;
            stopped_speaking_timeout = setTimeout(stopRecording, silence_secs * 1000);
          } else {
            clearTimeout(stopped_speaking_timeout);
          }
        }
      });
      console.error = origConsoleError; // Restore original console.error()
      myvad.start();

      document.getElementById('start').textContent = 'Start Recording';
      document.getElementById('start').disabled = false;

      } catch (e) {
        let contextInfo = context ? 
          `AudioContext state: ${context.state}, rate: ${context.sampleRate}`
          : 'AudioContext not initialized';
        let streamInfo = stream ?
          `MediaStream active: ${stream.active}, tracks: ${stream.getTracks().length}` 
          : 'MediaStream not initialized';
        let errorMessage = `Call stack: ${Error().stack}\n` +
                           `${contextInfo}\n${streamInfo}\n` +
                           `Browser: ${navigator.userAgent}`;
        console.error('Error initializing recorder:', e, '\n', errorMessage);
        alert('Error initializing recorder: ' + e + '\n' + errorMessage);
        location.reload();
      }
    }

    function startRecording() {
      setNoiseSuppression();
      recorder.port.postMessage({ message: 'UPDATE_RECORDING_STATE', 
                     setRecording: true });  // Begin recording
      isRecording = true;
      
      document.getElementById('start').textContent = 'End Recording';
      document.getElementById('startOver').style.display = 'inline';
      document.getElementById('vadSpan').textContent = '';
    }

    function stopRecording() {
      clearTimeout(stopped_speaking_timeout);
      document.getElementById('start').disabled = true;
      document.getElementById('startOver').disabled = true;
      recorder.port.postMessage({ message: 'UPDATE_RECORDING_STATE', 
                     setRecording: false });  // Halt recording
      isRecording = false;
    }

    // Setter for the noise suppression checkbox
    function setNoiseSuppression() {
      if (stream) {
        let audioTrack = stream.getAudioTracks()[0];
        if (audioTrack) {
          audioTrack.applyConstraints({ noiseSuppression: document.getElementById('suppressNoise').checked })
            .then(() => {
              let currentSettings = audioTrack.getSettings();
              if (currentSettings.noiseSuppression !== document.getElementById('suppressNoise').checked) {
                console.warn('Warning: Noise suppression setting did not apply as expected.');
                document.getElementById('suppressNoise').checked = currentSettings.noiseSuppression;
              }
            })
            .catch(err => {
              console.error('Failed to toggle noise suppression:', err);
              document.getElementById('suppressNoise').checked = !document.getElementById('suppressNoise').checked;
            });
        }
      }
    }
  
  </script>

  <br><br>
  
  Python Flask and JavaScript source code is
  <a href="https://replit.com/@jsalsman/webrec">on Replit</a>
  and <a href="https://github.com/jsalsman/webrec">GitHub.</a>
</body>
</html>
