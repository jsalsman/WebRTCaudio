<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Record Audio</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- maybe try shrink-to-fit=no and/or viewport-fit=cover for mobile -->
  <link rel="icon" href="/static/recbutton.jpg">
  <script src="/static/hark-bundle.js"></script>
</head>
<body>
  <h1>Record Audio with Stop on Silence</h1>

  <button id="start" style="font-size: 180%;">Start Recording</button>
  &nbsp;&nbsp;
  <button id="startOver" style="font-size: 180%; display: none;">Start Over</button>

  <br><br>
  Audio&nbsp;level:&nbsp;<meter id="level" min="0" max="1" 
    low="0.2" high="0.8" value="0" style="width: 270px;"></meter>
  <br>
  <span id="secs">00.0</span>&nbsp;seconds:&nbsp;<meter id="full" 
    min="0" max="60" high="55" value="0" style="width: 263px;"></meter>
  <br><br>

  <div style="font-size: 150%;">
    <label><input type="checkbox" id="vad" checked> Stop</label> after:
    <label><input type="radio" name="sil_dur" value="1"> 1,</label>
    <label><input type="radio" name="sil_dur" value="2" checked> 2,</label> or
    <label><input type="radio" name="sil_dur" value="3"> 3 seconds</label>
    of silence.
  </div>

  <script>
    var isRecording = false, recorder = null, context, stream, buffer, length;
    var stopped_speaking_timeout = null, speechEvents = null;

    document.getElementById('start').addEventListener('click', async () => {
      if (!isRecording) {
        startRecording();
      } else {
        stopRecording();
      }
    });

    async function startRecording() {
      if (!window.audioContext) {
        try {
          window.AudioContext = 
            window.AudioContext || window.webkitAudioContext;
          context = new AudioContext({ sampleRate: 16000 });
        } catch (e) {
          alert('Web Audio API not supported.');
          return;
        }
      }
      try {
        stream = await navigator.mediaDevices.getUserMedia(
          { audio: {
            echoCancellation: false,
            autoGainControl: false,   // TODO: try
            noiseSuppression: false,  // TODO: try
            latency: 0},              // TODO?
           video: false });
        
        const micSourceNode = new MediaStreamAudioSourceNode(context,
            {mediaStream: stream});
        
        await context.audioWorklet.addModule('/static/recording-processor.js');
        const recordingProperties = {
            numberOfChannels: 1,
            sampleRate: 16000,
            maxFrameCount: 16000 * 60 // one minute, adjust as needed
        };
        recorder = new AudioWorkletNode(context, 'recording-processor', 
          { processorOptions: recordingProperties });

        micSourceNode.connect(recorder);

        recorder.port.onmessage = event => {
          
          if (event.data.message === 'SHARE_RECORDING_BUFFER') { 
            // Recording ended, upload file
            console.log('Seconds Recorded: ' + event.data.recordingLength / 16000); // DEBUG

            let pcmData = new Uint8Array(event.data.recordingLength * 2);
            //let sum = 0.0; // DEBUG
            for (let index = 0; index < event.data.recordingLength; ++index) {
              let sample = event.data.buffer[0][index];
              //sum += Math.abs(sample);  // DEBUG
              sample = sample * 32768.0;
              sample = Math.max(-32768, Math.min(32767, sample));
              pcmData[index * 2] = sample & 255; // low byte, little endian
              pcmData[index * 2 + 1] = (sample >> 8) & 255; // high byte
            }
            //alert('Mean absolute sample: ' + (sum / event.data.recordingLength) + // DEBUG
            //      ' = sum: ' + sum + ' / samples: ' + event.data.recordingLength);

            let formData = new FormData();
            let blob = new Blob([pcmData], { type: 'audio/l16' });
            formData.append('audio', blob, 'audio.raw');
            fetch('/upload-audio', {
              method: 'POST',
              body: formData
            }).then(response => {
              if (response.ok) {
                window.location.href = response.url;
              }
            }).catch(error => {
              alert('Upload error:' + error);
            });
            
          } else if (event.data.message === 'MAX_RECORDING_LENGTH_REACHED') {
            stopRecording();
            
          } else if (event.data.message === 'UPDATE_RECORDING_LENGTH') {
            //console.log('Samples so far: ' + event.data.recordingLength);
            let seconds = event.data.recordingLength / 16000
            document.getElementById('full').value = seconds.toFixed(2);
            document.getElementById('secs').textContent = 
              seconds.toFixed(1).padStart(4, '0');
            
          } else if (event.data.message === 'UPDATE_VISUALIZERS') {
            //console.log('visualizer gain: ', event.data.gain) // DEBUG
            document.getElementById('level').value = 
              Math.abs(event.data.gain).toFixed(2);
            
          }
        }
        
        // Setup hark.js for voice activity detection
        speechEvents = hark(stream, {});
        speechEvents.on('speaking', function() {
          console.log('Speaking started');
          clearTimeout(stopped_speaking_timeout);
        });
        speechEvents.on('stopped_speaking', function() {
          console.log('Speaking stopped');
          if (document.getElementById('vad').checked) {
            silence_secs = document.querySelector(
              'input[name="sil_dur"]:checked').value;
            stopped_speaking_timeout = setTimeout(stopRecording, 
                                                  silence_secs * 1000);
          } else {
            clearTimeout(stopped_speaking_timeout);
          }
        });

        recorder.port.postMessage({ message: 'UPDATE_RECORDING_STATE', 
                       setRecording: true });
        isRecording = true;
        
        document.getElementById('start').textContent = 'End Recording';
        document.getElementById('startOver').style.display = 'inline';
      } catch (e) {
        console.error('Error initializing recording:', e);
        alert('Error initializing recording: ' + e);
      }
    }

    document.getElementById('startOver').onclick = function() {
      if (recorder && isRecording) { 
        // setRecording: true will reset recorded samples to zero
        recorder.port.postMessage({ message: 'UPDATE_RECORDING_STATE', 
                       setRecording: true });
        clearTimeout(stopped_speaking_timeout);
      }
    };

    async function stopRecording() {
      clearTimeout(stopped_speaking_timeout);
      document.getElementById('start').disabled = true;
      document.getElementById('startOver').disabled = true;
      recorder.port.postMessage({ message: 'UPDATE_RECORDING_STATE', 
                     setRecording: false });
      speechEvents.stop();
      isRecording = false;
    }
  </script>

  <br>
  Python Flask and JavaScript source code is
  <a href="https://replit.com/@jsalsman/WebRTCaudio">on Replit</a>
  and <a href="https://github.com/jsalsman/WebRTCaudio">GitHub.</a>
</body>
</html>
